{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse_norm = v2.Compose([\n",
    "#     v2.Normalize(mean=[0., 0., 0.], std=[1 / 0.2023, 1 / 0.1994, 1 / 0.2010]),\n",
    "#     v2.Normalize(mean=(-0.4914, -0.4822, -0.4465), std=[1., 1., 1.]),\n",
    "# ])\n",
    "\n",
    "# for x, y in train_dataset:\n",
    "#     break\n",
    "\n",
    "# image = inverse_norm(x)\n",
    "# image = (image * 255).permute(1, 2, 0)\n",
    "# image = image.to(torch.int)\n",
    "\n",
    "# plt.imshow(image)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import v2\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import wandb\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "from utils import accuracy\n",
    "from utils import load_from_checkpoint\n",
    "from trainer import Trainer\n",
    "from models import ResNet18\n",
    "import settings as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = Path(\"../data\")\n",
    "logs_path = Path(\"../logs\")\n",
    "logs_path.mkdir(exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "test_transforms = v2.Compose([\n",
    "    # Normalize\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_dataset = CIFAR10(data_path, train=False, transform=test_transforms, download=True)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test(model, test_dataloader, device):\n",
    "    model.to(device)\n",
    "    step_test_losses = []\n",
    "    step_test_accuracies = []\n",
    "\n",
    "    model.eval()\n",
    "    num_batches = len(test_dataloader)\n",
    "    for index, batch in enumerate(test_dataloader, start=1):\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        acc = accuracy(logits, y)\n",
    "\n",
    "        step_test_loss = loss.item()\n",
    "        step_test_accuracy = acc.item()\n",
    "        step_test_losses.append(step_test_loss)\n",
    "        step_test_accuracies.append(step_test_accuracy)\n",
    "\n",
    "        print(f\"Batch: {index}/{num_batches}, accuracy: {step_test_accuracy:.2f}\")\n",
    "\n",
    "    test_loss = torch.tensor(step_test_losses).mean()\n",
    "    test_accuracy = torch.tensor(step_test_accuracies).mean()\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Downloading large artifact run-5y973ba9-best_val_acc_93.97.pt:v0, 318.47MB. 1 files... \n",
      "wandb:   1 of 1 files downloaded.  \n",
      "Done. 0:0:1.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "ResNet18                                           [1024, 10]                --\n",
       "├─Sequential: 1-1                                  [1024, 512, 1, 1]         27,811,392\n",
       "├─Sequential: 1-2                                  [1024, 10]                5,130\n",
       "====================================================================================================\n",
       "Total params: 27,816,522\n",
       "Trainable params: 27,816,522\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.TERABYTES): 2.82\n",
       "====================================================================================================\n",
       "Input size (MB): 12.58\n",
       "Forward/backward pass size (MB): 17851.04\n",
       "Params size (MB): 111.27\n",
       "Estimated Total Size (MB): 17974.89\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api = wandb.Api()\n",
    "run = api.run(\"sampath017/ImageClassification/24z2beff\")\n",
    "artifact = api.artifact('sampath017/ImageClassification/run-5y973ba9-best_val_acc_93.97.pt:v0', type='model')\n",
    "local_path = artifact.download(root=logs_path)\n",
    "checkpoint = torch.load(Path(local_path)/\"best_val_acc_93.97.pt\", weights_only=True, map_location=device)\n",
    "\n",
    "model = ResNet18(num_classes=10)\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "summary(\n",
    "    model,\n",
    "    input_size=(1024, *test_dataset[0][0].shape),\n",
    "    device=\"cpu\",\n",
    "    mode=\"train\",\n",
    "    depth=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1/10, accuracy: 92.68\n",
      "Batch: 2/10, accuracy: 91.99\n",
      "Batch: 3/10, accuracy: 94.24\n",
      "Batch: 4/10, accuracy: 90.92\n",
      "Batch: 5/10, accuracy: 92.38\n",
      "Batch: 6/10, accuracy: 92.58\n",
      "Batch: 7/10, accuracy: 92.87\n",
      "Batch: 8/10, accuracy: 93.16\n",
      "Batch: 9/10, accuracy: 92.68\n",
      "Batch: 10/10, accuracy: 93.62\n",
      "\n",
      "loss=0.30, accuracy=92.71\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = test(model, test_dataloader, device)\n",
    "print(f\"\\n{loss=:.2f}, {accuracy=:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
